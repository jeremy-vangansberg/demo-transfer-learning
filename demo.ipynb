{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 391 entries, 0 to 390\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   391 non-null    int64 \n",
      " 1   url          391 non-null    object\n",
      " 2   title        391 non-null    object\n",
      " 3   description  390 non-null    object\n",
      " 4   cat1         391 non-null    object\n",
      " 5   cat2         318 non-null    object\n",
      " 6   cat3         135 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 21.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    " # Remplacer les NaN par des chaînes vides\n",
    "df = df.fillna('')\n",
    "\n",
    "# Concaténer les colonnes en une seule\n",
    "df['combined'] = df.apply(lambda row: '_'.join([row['cat1'], row['cat2'], row['cat3']]), axis=1)\n",
    "\n",
    "# Appliquer le one-hot encoding sur la colonne combinée\n",
    "one_hot_combined = df['combined'].str.get_dummies(sep='_')\n",
    "\n",
    "y = one_hot_combined.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = one_hot_combined.to_numpy()\n",
    "\n",
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['title'] + ' ' + df['description']\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremyvangansbeg/Documents/project/nlp-transfer-bce/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some layers from the model checkpoint at jplu/tf-camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFCamembertModel were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " camembert_embedding_layer   (None, 128, 768)             1106219   ['input_ids[0][0]',           \n",
      " (CamembertEmbeddingLayer)                                52         'attention_mask[0][0]']      \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['camembert_embedding_layer[0]\n",
      " SlicingOpLambda)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   24608     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 24)                   792       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110647352 (422.09 MB)\n",
      "Trainable params: 25400 (99.22 KB)\n",
      "Non-trainable params: 110621952 (421.99 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFCamembertModel, CamembertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Charger le tokenizer et le modèle Camembert\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "camembert_model = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "\n",
    "for layer in camembert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Définir les cibles (nb_targets) comme le nombre de colonnes dans y\n",
    "nb_targets = y.shape[1]\n",
    "\n",
    "# Définir la longueur maximale de la séquence\n",
    "max_seq_length = 128\n",
    "\n",
    "# Créer une classe de couche personnalisée pour encapsuler Camembert\n",
    "class CamembertEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, camembert_model, **kwargs):\n",
    "        super(CamembertEmbeddingLayer, self).__init__(**kwargs)\n",
    "        self.camembert_model = camembert_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.camembert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "# Définir les entrées du modèle\n",
    "input_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Utiliser la couche personnalisée pour obtenir les embeddings\n",
    "embeddings = CamembertEmbeddingLayer(camembert_model)([input_ids, attention_mask])\n",
    "\n",
    "# Utiliser le premier token (CLS) comme représentation de la séquence\n",
    "cls_token = embeddings[:, 0, :]\n",
    "\n",
    "# Ajouter une couche dense intermédiaire\n",
    "intermediate_layer = Dense(32, activation=\"relu\")(cls_token)\n",
    "\n",
    "# Ajouter la couche de sortie avec une activation sigmoid pour multi-label classification\n",
    "output = Dense(nb_targets, activation=\"sigmoid\")(intermediate_layer)\n",
    "\n",
    "# Créer le modèle\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compiler le modèle avec la loss binary_crossentropy\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fonction pour encoder les textes\n",
    "def encode_texts(texts, tokenizer, max_seq_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
    "        input_ids.append(encoded[\"input_ids\"][0].numpy())\n",
    "        attention_masks.append(encoded[\"attention_mask\"][0].numpy())\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "# Encoder les données textuelles d'entraînement et de test\n",
    "X_train_nlp_encoded, X_train_attention_masks = encode_texts(X_train, tokenizer, max_seq_length=max_seq_length)\n",
    "X_test_nlp_encoded, X_test_attention_masks = encode_texts(X_test, tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.6088 - accuracy: 0.0478 - val_loss: 0.5375 - val_accuracy: 0.0306\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4912 - accuracy: 0.0512 - val_loss: 0.4437 - val_accuracy: 0.0306\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4098 - accuracy: 0.0512 - val_loss: 0.3847 - val_accuracy: 0.0306\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3594 - accuracy: 0.0512 - val_loss: 0.3482 - val_accuracy: 0.0306\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3283 - accuracy: 0.0614 - val_loss: 0.3253 - val_accuracy: 0.1020\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3079 - accuracy: 0.1570 - val_loss: 0.3106 - val_accuracy: 0.0510\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2950 - accuracy: 0.1365 - val_loss: 0.3010 - val_accuracy: 0.0510\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2868 - accuracy: 0.1092 - val_loss: 0.2951 - val_accuracy: 0.0102\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2812 - accuracy: 0.0648 - val_loss: 0.2905 - val_accuracy: 0.0102\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2776 - accuracy: 0.0444 - val_loss: 0.2871 - val_accuracy: 0.0408\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2750 - accuracy: 0.0307 - val_loss: 0.2856 - val_accuracy: 0.0204\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2734 - accuracy: 0.0717 - val_loss: 0.2837 - val_accuracy: 0.0204\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2720 - accuracy: 0.0410 - val_loss: 0.2830 - val_accuracy: 0.0102\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2708 - accuracy: 0.0785 - val_loss: 0.2821 - val_accuracy: 0.0408\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2694 - accuracy: 0.1570 - val_loss: 0.2813 - val_accuracy: 0.1020\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2685 - accuracy: 0.1092 - val_loss: 0.2806 - val_accuracy: 0.0714\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2676 - accuracy: 0.1263 - val_loss: 0.2793 - val_accuracy: 0.1429\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2666 - accuracy: 0.1706 - val_loss: 0.2779 - val_accuracy: 0.1531\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2657 - accuracy: 0.1399 - val_loss: 0.2769 - val_accuracy: 0.1327\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2644 - accuracy: 0.1433 - val_loss: 0.2761 - val_accuracy: 0.1122\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2640 - accuracy: 0.1638 - val_loss: 0.2751 - val_accuracy: 0.1224\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2628 - accuracy: 0.1706 - val_loss: 0.2744 - val_accuracy: 0.1633\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2617 - accuracy: 0.1843 - val_loss: 0.2734 - val_accuracy: 0.1633\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2607 - accuracy: 0.1706 - val_loss: 0.2735 - val_accuracy: 0.1531\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2597 - accuracy: 0.1638 - val_loss: 0.2729 - val_accuracy: 0.1327\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2588 - accuracy: 0.1229 - val_loss: 0.2713 - val_accuracy: 0.1429\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2577 - accuracy: 0.1331 - val_loss: 0.2692 - val_accuracy: 0.1837\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2565 - accuracy: 0.1536 - val_loss: 0.2684 - val_accuracy: 0.1837\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2555 - accuracy: 0.1877 - val_loss: 0.2687 - val_accuracy: 0.1837\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2544 - accuracy: 0.1877 - val_loss: 0.2682 - val_accuracy: 0.1633\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2535 - accuracy: 0.1911 - val_loss: 0.2674 - val_accuracy: 0.1735\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2527 - accuracy: 0.1741 - val_loss: 0.2656 - val_accuracy: 0.1735\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2513 - accuracy: 0.1536 - val_loss: 0.2645 - val_accuracy: 0.1633\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2505 - accuracy: 0.1672 - val_loss: 0.2637 - val_accuracy: 0.1735\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2491 - accuracy: 0.1775 - val_loss: 0.2629 - val_accuracy: 0.1531\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2482 - accuracy: 0.1877 - val_loss: 0.2615 - val_accuracy: 0.1633\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2472 - accuracy: 0.1945 - val_loss: 0.2607 - val_accuracy: 0.1735\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2461 - accuracy: 0.1877 - val_loss: 0.2604 - val_accuracy: 0.1633\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2450 - accuracy: 0.1843 - val_loss: 0.2600 - val_accuracy: 0.1633\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2440 - accuracy: 0.1980 - val_loss: 0.2590 - val_accuracy: 0.1633\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2430 - accuracy: 0.1741 - val_loss: 0.2572 - val_accuracy: 0.1735\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2418 - accuracy: 0.1775 - val_loss: 0.2561 - val_accuracy: 0.1735\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.2407 - accuracy: 0.1877 - val_loss: 0.2555 - val_accuracy: 0.1837\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2399 - accuracy: 0.1843 - val_loss: 0.2546 - val_accuracy: 0.1837\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2389 - accuracy: 0.1877 - val_loss: 0.2533 - val_accuracy: 0.1837\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2380 - accuracy: 0.1706 - val_loss: 0.2529 - val_accuracy: 0.1020\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2369 - accuracy: 0.1706 - val_loss: 0.2519 - val_accuracy: 0.1735\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2357 - accuracy: 0.1877 - val_loss: 0.2509 - val_accuracy: 0.1633\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2347 - accuracy: 0.1843 - val_loss: 0.2508 - val_accuracy: 0.1735\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2337 - accuracy: 0.1877 - val_loss: 0.2495 - val_accuracy: 0.1633\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.2495 - accuracy: 0.1633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24946370720863342, 0.16326530277729034]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "model.fit([X_train_nlp_encoded, X_train_attention_masks], y_train, epochs=50, batch_size=32, validation_data=([X_test_nlp_encoded, X_test_attention_masks], y_test))\n",
    "\n",
    "# Évaluer le modèle\n",
    "model.evaluate([X_test_nlp_encoded, X_test_attention_masks], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 10s 2s/step\n"
     ]
    }
   ],
   "source": [
    "test1 = 'Brocante de Valenciennes - Ceci est une brocante de meubles et de vêtements'\n",
    "\n",
    "\n",
    "to_test = encode_texts(texts=test1, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "proba = model.predict(to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 164ms/step\n",
      "Indices des 3 plus grandes probabilités : [23  7 20]\n",
      "Valeurs des 3 plus grandes probabilités : [0.38642192 0.23983611 0.14258905]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour encoder un seul texte\n",
    "def encode_single_text(text, tokenizer, max_seq_length):\n",
    "    encoded = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Exemple de texte à tester\n",
    "test1 = 'balade au musée du Louvre - Venez découvrir les plus belles œuvres du Louvre'\n",
    "\n",
    "# Encoder le texte\n",
    "input_ids, attention_mask = encode_single_text(test1, tokenizer, max_seq_length)\n",
    "\n",
    "# Faire la prédiction\n",
    "proba = model.predict([input_ids, attention_mask])\n",
    "\n",
    "# Convertir les probabilités en un tableau numpy\n",
    "proba = np.squeeze(proba)  # Assurez-vous que la dimension est correcte\n",
    "\n",
    "# Obtenir les indices des 3 plus grandes probabilités\n",
    "top_3_indices = np.argsort(proba)[-3:][::-1]\n",
    "\n",
    "# Afficher les indices des 3 classes avec les plus grandes probabilités\n",
    "print(\"Indices des 3 plus grandes probabilités :\", top_3_indices)\n",
    "\n",
    "# Afficher les valeurs des 3 plus grandes probabilités\n",
    "top_3_probabilities = proba[top_3_indices]\n",
    "print(\"Valeurs des 3 plus grandes probabilités :\", top_3_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Visite', 'Culture', 'Spectacle'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_combined.columns[top_3_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
